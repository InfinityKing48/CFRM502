---
title: "502_HW7"
author: "Shengbo Jin"
date: "3/2/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Q1

```{r}
load("Homework 7 Data.Rdata")
head(futures)
attach(futures)
```

### (a)

```{r}
Y <- diff(lnfuture)
X <- diff(lnspot)
lm <- lm(Y~X)
summary(lm)
res.lm <- resid(lm)
acf(res.lm)
```

```{r}
Box.test(res.lm, lag=10, type="Ljung-Box", fitdf=2)
```

A plot of the sample autocorrelation function shows the residuals have significant autocorrelation, and a Ljung-Box test also confirms this.

### (b)

```{r}
library(forecast)
auto.arima(res.lm)
```

```{r}
(mod.arimax1 <- Arima(Y, xreg=X, order=c(1,0,1)))
```

```{r}
(mod.arimax2 <- auto.arima(Y, xreg=X))
```

The fitted model is ARIMAX(1,0,2), and has lower AICc than the previous model as expected.

```{r}
checkresiduals(mod.arimax2)
```

To a large extent, both plots and tests show the residuals are white noise, so the model is a good fit.

### (c)

```{r}
forecast(mod.arimax2, xreg=0.0002, level=0.95)
```

## Q2

```{r}
data(SP500, package="Ecdat")
ret <- SP500$r500[(1804-2*253+1):1804]
ret.black.mon <- SP500$r500[1805]
```

### (a)

```{r}
(mean.fit <- auto.arima(ret))
```

```{r}
checkresiduals(mean.fit)
```

```{r}
acf(resid(mean.fit)^2, main="")
pacf(resid(mean.fit)^2, main="")
```

The residuals of the fitted MA(1) appear to be white noise, but have volatility clustering as the time series plot and sample acf of squared residuals shows. Sample pacf of squared residuals suggests a GARCH(1,1) model.

### (b)

```{r warning=FALSE}
library(fGarch)
fit.armagarch <- garchFit(~arma(0,1)+garch(1,1), data=ret, 
                          cond.dist = "std", trace=FALSE)
summary(fit.armagarch)
```

```{r}
plot(fit.armagarch, which=10)
plot(fit.armagarch, which=11)
plot(fit.armagarch, which=13)
```

According to the Ljung-Box test on R, for lags up 10, 15, 20, we fail to reject zero autocorrelation; According to the Ljung-Box test on R\^2, for lags up 10, 15, 20, we fail to reject zero autocorrelation. Based on the LM Arch test, we fail to reject the null that all ARCH coefficients of the standardized residuals are 0. All these test results and plots suggest that the model is a good fit for capturing the volatility clustering.

### (c)

```{r}
(armagarch.pred <- predict(fit.armagarch))
```

```{r}
(a <- armagarch.pred[1, 1])
(b <- armagarch.pred[1, 2])
(v <- coef(fit.armagarch)["shape"])
```

### (d)

```{r}
q <- as.numeric(qstd(0.001, nu=coef(fit.armagarch)["shape"]))
(VaR <- -(q*b + a))
-(ret.black.mon) > VaR
```

### (e)

```{r}
plot(fit.armagarch@sigma.t, type='l', col='blue')
```

### (f)

```{r}
sigma <- rep(1, 506)
sigma[1] <- 0.013
at <- residuals(fit.armagarch, standardize=FALSE)

for (i in seq(2, 506)) {
  sigma[i] <- sqrt(coef(fit.armagarch)["omega"] + 
                   coef(fit.armagarch)["alpha1"]*at[i-1]**2 + 
                   coef(fit.armagarch)["beta1"]*sigma[i-1]**2)
}

plot(fit.armagarch@sigma.t, type='l', col='blue', ylab='')
par(new=TRUE)
plot(sigma, type='l', col='red')
```

### (g)

![](images/2.png)

## Q3

![](images/1.png)

## Q4

```{r}
library(fracdiff)
set.seed(343)

Y <- fracdiff.sim(200, ar=0.35, d=0.3, sd=1)$series
plot(Y, type="l", xlab="t", ylab="Y", main="ARFIMA(1,0.3,0)")
```

```{r}
(d0 <- coef(fracdiff(Y, nar=0, nma=0))[1])
(d1 <- coef(fracdiff(Y, nar=1, nma=1))[1])
```

```{r warning=FALSE}
d0_200 <- rep(1, 1000)
d1_200 <- rep(1, 1000)

for (i in seq(1,1000)) {
  Y <- fracdiff.sim(200, ar=0.35, d=0.3, sd=1)$series
  d0_200[i] <- coef(fracdiff(Y, nar=0, nma=0))[1]
  d1_200[i] <- coef(fracdiff(Y, nar=1, nma=1))[1]
}

hist(as.numeric(d0_200), breaks=20)
hist(as.numeric(d1_200), breaks=20)
```

```{r warning=FALSE}
d0_1000 <- rep(1, 1000)
d1_1000 <- rep(1, 1000)

for (i in seq(1,1000)) {
  Y <- fracdiff.sim(1000, ar=0.35, d=0.3, sd=1)$series
  d0_1000[i] <- coef(fracdiff(Y, nar=0, nma=0))[1]
  d1_1000[i] <- coef(fracdiff(Y, nar=1, nma=1))[1]
}

hist(as.numeric(d0_1000), breaks=20)
hist(as.numeric(d1_1000), breaks=20)
```

As the sample size increases, fitting an ARFIMA(1, d, 1) process is much better than fitting an ARFIMA(0, d, 0) in estimating d.
